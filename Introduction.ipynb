{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 书籍推荐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Grokking Deep Learning](https://www.manning.com/books/grokking-deep-learning) by Andrew Trask. Use our exclusive discount code traskud17 for 40% off. This provides a very gentle introduction to Deep Learning and covers the intuition more than the theory.\n",
    "\n",
    "- [Neural Networks And Deep Learning](http://neuralnetworksanddeeplearning.com/) by Michael Nielsen. This book is more rigorous than Grokking Deep Learning and includes a lot of fun, interactive visualizations to play with.\n",
    "\n",
    "- [The Deep Learning Textbook](http://www.deeplearningbook.org/) from Ian Goodfellow, Yoshua Bengio, and Aaron Courville. This online book contains a lot of material and is the most rigorous of the three books suggested.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron（神经单元）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用神经单元制作AND逻辑器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice!  You got it all correct.\n",
      "\n",
      " Input 1    Input 2    Linear Combination    Activation Output   Is Correct\n",
      "       0          0                  -1.5                    0          Yes\n",
      "       0          1                  -0.5                    0          Yes\n",
      "       1          0                  -0.5                    0          Yes\n",
      "       1          1                   0.5                    1          Yes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# TODO: Set weight1, weight2, and bias\n",
    "weight1 = 1.0\n",
    "weight2 = 1.0\n",
    "bias = -1.5\n",
    "\n",
    "# DON'T CHANGE ANYTHING BELOW\n",
    "# Inputs and outputs\n",
    "test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "correct_outputs = [False, False, False, True]\n",
    "outputs = []\n",
    "\n",
    "# Generate and check output\n",
    "for test_input, correct_output in zip(test_inputs, correct_outputs):\n",
    "    linear_combination = weight1 * test_input[0] + weight2 * test_input[1] + bias\n",
    "    output = int(linear_combination >= 0)\n",
    "    is_correct_string = 'Yes' if output == correct_output else 'No'\n",
    "    outputs.append([test_input[0], test_input[1], linear_combination, output, is_correct_string])\n",
    "\n",
    "# Print output\n",
    "num_wrong = len([output[4] for output in outputs if output[4] == 'No'])\n",
    "output_frame = pd.DataFrame(outputs, columns=['Input 1', '  Input 2', '  Linear Combination', '  Activation Output', '  Is Correct'])\n",
    "if not num_wrong:\n",
    "    print('Nice!  You got it all correct.\\n')\n",
    "else:\n",
    "    print('You got {} wrong.  Keep trying!\\n'.format(num_wrong))\n",
    "print(output_frame.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 异或XOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "逻辑为，AB相同则输出为0；AB相异则输出为1.\n",
    "\n",
    "表达式：\n",
    "\n",
    "`（非A 交 B）并（A 交 非B）`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 简单的分类（线性回归）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "伪代码：\n",
    "> a为学习速率\n",
    "\n",
    "- 使用随机的w与b，绘制分界线\n",
    "- 对于每个误分类的样本（x1...xn)执行：\n",
    "    - 如果y=1，但y_hat=0:\n",
    "        - w + ax\n",
    "        - b + a\n",
    "    - 如果y=0，但y_hat=1:\n",
    "        - w - ax\n",
    "        - b - a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Setting the random seed, feel free to change it and see different solutions.\n",
    "np.random.seed(42)\n",
    "\n",
    "def stepFunction(t):\n",
    "    if t >= 0:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def prediction(X, W, b):\n",
    "    return stepFunction((np.matmul(X,W)+b)[0])\n",
    "\n",
    "# TODO: Fill in the code below to implement the perceptron trick.\n",
    "# The function should receive as inputs the data X, the labels y,\n",
    "# the weights W (as an array), and the bias b,\n",
    "# update the weights and bias W, b, according to the perceptron algorithm,\n",
    "# and return W and b.\n",
    "def perceptronStep(X, y, W, b, learn_rate = 0.01):\n",
    "    for i in range(len(X)):\n",
    "        y_hat = prediction(X[i],W,b)\n",
    "        if y[i]-y_hat == 1:\n",
    "            W[0] += X[i][0]*learn_rate\n",
    "            W[1] += X[i][1]*learn_rate\n",
    "            b += learn_rate\n",
    "        elif y[i]-y_hat == -1:\n",
    "            W[0] -= X[i][0]*learn_rate\n",
    "            W[1] -= X[i][1]*learn_rate\n",
    "            b -= learn_rate\n",
    "    return W, b\n",
    "    \n",
    "# This function runs the perceptron algorithm repeatedly on the dataset,\n",
    "# and returns a few of the boundary lines obtained in the iterations,\n",
    "# for plotting purposes.\n",
    "# Feel free to play with the learning rate and the num_epochs,\n",
    "# and see your results plotted below.\n",
    "def trainPerceptronAlgorithm(X, y, learn_rate = 0.01, num_epochs = 25):\n",
    "    x_min, x_max = min(X.T[0]), max(X.T[0])\n",
    "    y_min, y_max = min(X.T[1]), max(X.T[1])\n",
    "    W = np.array(np.random.rand(2,1))\n",
    "    b = np.random.rand(1)[0] + x_max\n",
    "    # These are the solution lines that get plotted below.\n",
    "    boundary_lines = []\n",
    "    for i in range(num_epochs):\n",
    "        # In each epoch, we apply the perceptron step.\n",
    "        W, b = perceptronStep(X, y, W, b, learn_rate)\n",
    "        boundary_lines.append((-W[0]/W[1], -b/W[1]))\n",
    "    return boundary_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/perceptron_data.csv',names=['X1','X2','y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.78051</td>\n",
       "      <td>-0.063669</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.28774</td>\n",
       "      <td>0.291390</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.40714</td>\n",
       "      <td>0.178780</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.29230</td>\n",
       "      <td>0.421700</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.50922</td>\n",
       "      <td>0.352560</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.77029</td>\n",
       "      <td>0.701400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.73156</td>\n",
       "      <td>0.717820</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.44556</td>\n",
       "      <td>0.579910</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.85275</td>\n",
       "      <td>0.859870</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.51912</td>\n",
       "      <td>0.623590</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         X1        X2  y\n",
       "0   0.78051 -0.063669  1\n",
       "1   0.28774  0.291390  1\n",
       "2   0.40714  0.178780  1\n",
       "3   0.29230  0.421700  1\n",
       "4   0.50922  0.352560  1\n",
       "..      ...       ... ..\n",
       "95  0.77029  0.701400  0\n",
       "96  0.73156  0.717820  0\n",
       "97  0.44556  0.579910  0\n",
       "98  0.85275  0.859870  0\n",
       "99  0.51912  0.623590  0\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([1.32111616]), array([3.39038455])),\n",
       " (array([-0.02727335]), array([0.58093127])),\n",
       " (array([-0.06953723]), array([0.5033617])),\n",
       " (array([-0.09336359]), array([0.50938365])),\n",
       " (array([-0.11776695]), array([0.51555142])),\n",
       " (array([-0.14276851]), array([0.52187039])),\n",
       " (array([-0.16839056]), array([0.52834619])),\n",
       " (array([-0.19465648]), array([0.53498471])),\n",
       " (array([-0.20547521]), array([0.57024877])),\n",
       " (array([-0.22495207]), array([0.57724872])),\n",
       " (array([-0.24289533]), array([0.58594893])),\n",
       " (array([-0.26138775]), array([0.5949154])),\n",
       " (array([-0.28045492]), array([0.60416056])),\n",
       " (array([-0.30012406]), array([0.6136976])),\n",
       " (array([-0.32842636]), array([0.5900821])),\n",
       " (array([-0.34182866]), array([0.63222086])),\n",
       " (array([-0.37108442]), array([0.6079341])),\n",
       " (array([-0.38612915]), array([0.65189711])),\n",
       " (array([-0.4164041]), array([0.62689996])),\n",
       " (array([-0.44025201]), array([0.63605195])),\n",
       " (array([-0.44738156]), array([0.67836799])),\n",
       " (array([-0.46823366]), array([0.64805537])),\n",
       " (array([-0.4761619]), array([0.69147238])),\n",
       " (array([-0.49729176]), array([0.66052055])),\n",
       " (array([-0.51307359]), array([0.70672918]))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainPerceptronAlgorithm(data[['X1','X2']].values,data['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 非线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有些场景下的分类，不能通过一条直线进行很好的划分，有时需要曲线。这时候，前面的方法可能就不太适用了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error Function\n",
    "\n",
    "Error Function可以帮忙给出我们与实际的误差有多少，然后进行不断迭代优化。他应该满足如下两个条件：\n",
    "- 是一个连续值（continuous）。（因为我们要判定优化的方向，不能是离散值）\n",
    "- 可微分（differentiable）。（最优的方向，即导数方向，所以需要可微）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前面了解到，为了方便不断优化，Error Function需要是一个连续值，所以，我们也需要**将预测结果转换为连续值**。\n",
    "\n",
    "常用的一种方式就是，将前面提到的Activation Function由阶跃函数（Step Function）替换为Sigmoid Function，如下所示：\n",
    "<img src=\"imgs/01.png\" width=600px>\n",
    "\n",
    "<img src=\"imgs/02.png\" width=600px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前面我们都是查看的二分类情况，如果有多分类时该如何处理呢？比如说，我们想区分出猫、狗与兔子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这时候，我们需要再次优化`Activation Function`，它可以把方程输出不同类目的score转换为不同类目的probability，比如三个分类的score分别是：\n",
    "\n",
    "```\n",
    "猫：2\n",
    "狗：1\n",
    "兔：0\n",
    "```\n",
    "如何把他们转换为相加为1的概率呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个朴素的想法就是“归一”，即将所有类目的`score / sum(score）`，得到：\n",
    "```\n",
    "猫：2/(2+1)\n",
    "狗：1/（2+1）\n",
    "兔：0/（2+1）\n",
    "```\n",
    "这样确实满足了：转化为概率，且概率和为1.但似乎忽略了一个问题，因为我们的**方程输出结果并不一定全部为正数**，假如兔的score为-3，那么兔的概率计算中，就会出现分母为0的情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但这个思路是正确的，我们只需要把所有的计算项，都强制转化为正数，这时候想到了可以使用`exp`函数，那么我们就得到了：\n",
    "```\n",
    "p_1 = exp(s_1)/(exp(s_1)+exp(s_2)+...+exp(s_n))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如上方程我们也称之为**Softmax Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python实现\n",
    "import numpy as np\n",
    "\n",
    "def Softmax(L):\n",
    "    \n",
    "    expL = np.exp(L)    \n",
    "    result = [i/sum(expL) for i in expL]\n",
    "\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最大似然"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 什么是最大似然"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于这么多模型，我们如何选出最佳的那个呢？\n",
    "\n",
    "因为我们处理的模型产出，都是“概率”，那么，我们就选出实际情况下对应概率最大的那个模型就好了，这种方法叫做**Maximum Likelihood**，最大似然估计方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设，我们有两个模型，分类结果分别如下：\n",
    "<img src=\"imgs/03.png\" width=600px>\n",
    "很明显，我们能看出来，右边的模型要比左边的好（因为分对了更多类别），那么，如何从“概率”上来阐述这件事呢？"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于左图而言，我们可以分别得到 **各类别符合实际情况的概率**，如下所示：\n",
    "<img src=\"imgs/04.png\" width=300px>\n",
    "那么，整体分类正确的概率就是：`0.6 x 0.2 x 0.1 x 0.7 = 0.0084`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同理，对于右图，我们得到的整体分类概率是：`0.7 x 0.9 x 0.8 x 0.6 = 0.3024`\n",
    "\n",
    "<img src=\"imgs/05.png\" width=300px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么，我们在进行模型优化的时候，就可以通过“最大化分类概率”来实现，即**Maximum Likelihood**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 如何利用最大似然优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还记得前面我们定义了Error Function么？最大化概率是不是就代表着最小化误差呢？想想看"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在进行最大化概率时，前面我们使用了 **乘法**，但如果有成千上万个样本，得到的积数会是一个非常非常小的值，而且计算起来比较麻烦。\n",
    "\n",
    "而**求和**，就能好一些，那么，如何将“乘法”转换为“加法”呢？\n",
    "\n",
    "答案是：使用**log**函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那这样，我们就可以把如上概率相乘的计算转为如下所示，因为0～1的ln值都是负数，所以 我们再使用负号，将结果转为正数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/06.png\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如上最终得到的计算结果，我们称之为“**交叉熵（Cross Entropy）**”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 交叉熵越大，系统越不稳定，模型效果越差\n",
    "- 交叉熵越小，则反之"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同样，对于每一个样本，误差越大，得到的Cross Entropy也就越大，如下所示：\n",
    "\n",
    "<img src=\"imgs/07.png\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**所以，我们找到了probability与Error Function之间的关系，那就是Cross Entropy，我们可以通过优化它来达到模型优化的目的**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "167px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
